# Benchmarking the Trustworthiness in Multimodal LLMs for Video Understanding

![](./docs/structure/logo.jpg)



***

**Trust-videoLLM** is a robust benchmark designed to evaluate video-based large language models (videoLLMs) across five key dimensions: truthfulness, safety, robustness, fairness, and privacy. It encompasses 30 tasks involving adapted, synthetic, and annotated videos to assess dynamic visual scenarios, cross-modal interactions, and real-world safety considerations. Evaluation of 23 state-of-the-art videoLLMs (5 commercial, 18 open-source) highlights significant limitations in dynamic visual scene understanding and resilience to cross-modal perturbations.

![](./docs/structure/framewrok.png)

### News
-  **2025.11.08 ğŸ‰ ğŸ‰ ğŸ‰ [Our paper](https://arxiv.org/abs/2506.12336) has been accepted by the AAAI 2026 (Oral) ï¼See you in Singapore ~**

### VideoLLMs Evaluation Results Ranking
***
![](./docs/structure/rank.png)

### Dependencies
***
1. Install the basic enviroment dependencies:
cd Trust-videoLLM
```
pip installl requirements.txt
```
2. Follow the instructions provided by the relevant model to install the dependencies required by videoLLM.

### Datasets
***
This dataset contains potentially offensive or disturbing content, including but not limited to pornography, violence, and graphic videos. Researchers requiring access to the dataset must contact wangyouze6889@163.com for authorization.

### Run
***

**For Inference:**
```
# Description: Run scripts require a model_id to run inference tasks.
# Usage: bash scripts/run/*/*.sh <model_id>

scripts/run
â”œâ”€â”€ fairness_scripts
â”‚   â”œâ”€â”€ f1-stereotype-impact-generation.sh
â”‚   â”œâ”€â”€ f2-perference-video-selection.sh
â”‚   â”œâ”€â”€ f3-profession-prediction.sh
â”‚   â”œâ”€â”€ f4-agrement-on-stereotype.sh
â”‚   â””â”€â”€ f5-time-sensitivity-stereotype.sh
â”œâ”€â”€ privacy_scripts
â”‚   â”œâ”€â”€ p1-privacy-identification.sh
â”‚   â”œâ”€â”€ p2-privacy-vqa.sh
â”‚   â”œâ”€â”€ p3-infoflow-exception.sh
â”‚   â”œâ”€â”€ p4-celebrities.sh
â”‚   â””â”€â”€ p5-privacy-inference.sh
â”œâ”€â”€ robustness_scripts
â”‚   â”œâ”€â”€ r1-OOD-video-caption.sh
â”‚   â”œâ”€â”€ r2-noise-VQA.sh
â”‚   â”œâ”€â”€ r3-temporal-consistency.sh
â”‚   â”œâ”€â”€ r4-adversarial-attack-classification.sh
â”‚   â”œâ”€â”€ r5-adversarial-attack-captioning.sh
â”‚   â”œâ”€â”€ r6-impact-video-sentiment-analysis.sh
â”‚   â”œâ”€â”€ r7-adversarial-text.sh
â”‚   â””â”€â”€ r8-misleading-prompts.sh
â”œâ”€â”€ safety_scripts
â”‚   â”œâ”€â”€ s1-nsfw-video-generation.sh
â”‚   â”œâ”€â”€ s2-nsfw-prompt-execution.sh
â”‚   â”œâ”€â”€ s3-toxic-content-continues.sh
â”‚   â”œâ”€â”€ s4-identification-video-risky-content.sh
â”‚   â”œâ”€â”€ s5-temporal-dependency-misleading.sh
â”‚   â”œâ”€â”€ s6-deepfake-identification.sh
â”‚   â”œâ”€â”€ s7-figstep-jailbreak.sh
â”‚   â”œâ”€â”€ s7-mmsafetybench-jailbreak.sh
â”‚   â””â”€â”€ s7-videoJail-jailbreak.sh
â””â”€â”€ truthfulness_scripts
    â”œâ”€â”€ t1-vqa-contextual.sh
    â”œâ”€â”€ t2-vqa-temporal.sh
    â”œâ”€â”€ t3-video-caption.sh
    â”œâ”€â”€ t4-events-understanding.sh
    â””â”€â”€ t5-video-hallucination.sh
```

**For Evaluation:**

Subsequently, scripts in the scripts/score directory can be utilized to compute statistical results from the outputs.

```
# Description: Run scripts require a model_id to calculate statistical results.
# Usage: python scripts/score/*/*.py --model_id <model_id>

scripts/score
â”œâ”€â”€ fairness_scripts
â”‚   â”œâ”€â”€ f1-stereotype-impact-generation.py
â”‚   â”œâ”€â”€ f2-perference-video-selection.py
â”‚   â”œâ”€â”€ f3-profession-prediction.py
â”‚   â”œâ”€â”€ f4-agrement-on-stereotype.py
â”‚   â””â”€â”€ f5-time-sensitivity-stereotype.py
â”œâ”€â”€ privacy_scripts
â”‚   â”œâ”€â”€ p1-privacy-identification.py
â”‚   â”œâ”€â”€ p2-privacy-vqa.py
â”‚   â”œâ”€â”€ p3-infoflow-exception.py
â”‚   â”œâ”€â”€ p4-celebrities.py
â”‚   â””â”€â”€ p5-privacy-inference.py
â”œâ”€â”€ robustness_scripts
â”‚   â”œâ”€â”€ r1-OOD-video-caption.py
â”‚   â”œâ”€â”€ r2-noise-VQA.py
â”‚   â”œâ”€â”€ r3-temporal-consistency.py
â”‚   â”œâ”€â”€ r4-adversarial-attack-classification.py
â”‚   â”œâ”€â”€ r5-adversarial-attack-captioning.py
â”‚   â”œâ”€â”€ r6-impact-video-sentiment-analysis.py
â”‚   â”œâ”€â”€ r7-adversarial-text.py
â”‚   â””â”€â”€ r8-misleading-prompts.py
â”œâ”€â”€ safety_scripts
â”‚   â”œâ”€â”€ s1-nsfw-video-generation.py
â”‚   â”œâ”€â”€ s2-nsfw-prompt-execution.py
â”‚   â”œâ”€â”€ s3-toxic-content-continues.py
â”‚   â”œâ”€â”€ s4-identification-video-risky-content.py
â”‚   â”œâ”€â”€ s5-temporal-dependency-misleading.py
â”‚   â”œâ”€â”€ s6-deepfake-identification.py
â”‚   â”œâ”€â”€ s7-figstep-jailbreak.py
â”‚   â”œâ”€â”€ s7-mmsafetybench-jailbreak.py
â”‚   â””â”€â”€ s7-videoJail-jailbreak.py
â””â”€â”€ truthfulness_scripts
    â”œâ”€â”€ t1-vqa-contextual.py
    â”œâ”€â”€ t2-vqa-temporal.py
    â”œâ”€â”€ t3-video-caption.py
    â”œâ”€â”€ t4-events-understanding.py
    â””â”€â”€ t5-video-hallucination.py
```

### Tasks List
***

![](./docs/structure/tasks_list.png)


### Citation
If you find our work useful in your research, we kindly encourage you to cite our paper.
```
@article{wang2025understanding,
  title={Benchmarking the Trustworthiness in Multimodal LLMs for Video Understanding},
  author={Wang, Youze and Chen, Zijun and Chen, Ruoyu and Gu, Shishen and Dong, Yinpeng and Su, Hang and Zhu, Jun and Wang, Meng and Hong, Richang and Hu, Wenbo},
  journal={arXiv preprint arXiv:2506.12336},
  year={2025}
}
```